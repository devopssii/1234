{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/rmcpantoja/piper/blob/master/notebooks/piper_multilingual_training_notebook.ipynb","timestamp":1709921876058}]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7850287,"sourceType":"datasetVersion","datasetId":4603590},{"sourceId":7877249,"sourceType":"datasetVersion","datasetId":4622922},{"sourceId":7905960,"sourceType":"datasetVersion","datasetId":4643151}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font color=\"ffc800\"> **[Piper](https://github.com/rhasspy/piper) training notebook.**\n## ![Piper logo](https://contribute.rhasspy.org/img/logo.png)\n\n---\n\n- Notebook made by [rmcpantoja](http://github.com/rmcpantoja)\n- Collaborator: [Xx_Nessu_xX](http://github.com/Xx_Nessu_xX)\n\n---\n\n# Notes:\n\n- <font color=\"orange\">**Things in orange mean that they are important.**\n\n# Credits:\n\n* [Feanix-Fyre fork](https://github.com/Feanix-Fyre/piper) with some improvements.\n* [Tacotron2 NVIDIA training notebook](https://github.com/justinjohn0306/FakeYou-Tacotron2-Notebook) - Dataset duration snippet.\n* [üê∏TTS](https://github.com/coqui-ai/TTS) - Resampler and XTTS formater demo.","metadata":{"id":"eK3nmYDB6C1a"}},{"cell_type":"markdown","source":"# <font color=\"ffc800\">üîß ***First steps.*** üîß","metadata":{"id":"AICh6p5OJybj"}},{"cell_type":"code","source":"#@markdown ## <font color=\"ffc800\"> **Google Colab Anti-Disconnect.** üîå\n#@markdown ---\n#@markdown #### Avoid automatic disconnection. Still, it will disconnect after <font color=\"orange\">**6 to 12 hours**</font>.\n\nimport IPython\njs_code = '''\nfunction ClickConnect(){\nconsole.log(\"Working\");\ndocument.querySelector(\"colab-toolbar-button#connect\").click()\n}\nsetInterval(ClickConnect,60000)\n'''\ndisplay(IPython.display.Javascript(js_code))","metadata":{"cellView":"form","executionInfo":{"elapsed":321,"status":"ok","timestamp":1710412590021,"user":{"displayName":"Ab As","userId":"09234410323733171058"},"user_tz":-300},"id":"qyxSMuzjfQrz","outputId":"51b20480-4cae-4a4f-f63b-f506fb196620"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown ## <font color=\"ffc800\"> **Check GPU type.** üëÅÔ∏è\n#@markdown ---\n#@markdown #### A higher capable GPU can lead to faster training speeds. By default, you will have a <font color=\"orange\">**Tesla T4**</font>.\n!nvidia-smi","metadata":{"cellView":"form","executionInfo":{"elapsed":303,"status":"ok","timestamp":1710412598128,"user":{"displayName":"Ab As","userId":"09234410323733171058"},"user_tz":-300},"id":"ygxzp-xHTC7T","outputId":"ead17fb7-697a-41b7-d7ac-49cc36a7e9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install resemble-enhance --upgrade\nprint(\"Done install resemble-enhance!\")","metadata":{"executionInfo":{"elapsed":8850,"status":"ok","timestamp":1710412619673,"user":{"displayName":"Ab As","userId":"09234410323733171058"},"user_tz":-300},"id":"qZvbKlRF6j0_","outputId":"0817c437-018b-4ad9-d052-79022fa29688","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **Install resemble-enhance.** üì¶\n#@markdown ---\n#@markdown ####In this cell the synthesizer and its necessary dependencies to execute the training will be installed. (this may take a while)\n\n#!pip install resemble-enhance --upgrade\n#print(\"Done install resemble-enhance!\")\n\n#@markdown # <font color=\"ffc800\"> **1. Extract dataset.** üì•\n#@markdown ---\n#@markdown ####Important: the audios must be in <font color=\"orange\">**wav format, (16000 or 22050hz, 16-bits, mono), and, for convenience, numbered. Example:**\n\n#@markdown * <font color=\"orange\">**1.wav**</font>\n#@markdown * <font color=\"orange\">**2.wav**</font>\n#@markdown * <font color=\"orange\">**3.wav**</font>\n#@markdown * <font color=\"orange\">**.....**</font>\n\n#@markdown ---\nimport os\nimport wave\nimport zipfile\nimport datetime\n\n#%cd /content\n#if not os.path.exists(\"/content/drive/MyDrive/TTS_UZB\"):\n#    os.makedirs(\"/content/drive/MyDrive/TTS_UZB\")\n#    os.makedirs(\"/content/drive/MyDrive/TTS_UZB/prepwavs\")\n#    os.makedirs(\"/content/drive/MyDrive/TTS_UZB/prepwavs/wavs\")\n%cd /content/drive/MyDrive/TTS_UZB/\n#@markdown ### Audio dataset path to unzip:\n# –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ ZIP-–∞—Ä—Ö–∏–≤—É —Å –∞—É–¥–∏–æ\nzip_path = \"/kaggle/input/uzaudio/prepwavs.zip\"  # @param {type:\"string\"}\nzip_path = zip_path.strip()\n\n# –†–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ–º –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã\nif zip_path:\n    if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n        print(\"Unzipping audio content...\")\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(\"/kaggle/working/\")\n    else:\n        raise Exception(\"The path provided is not correct or not a zip file. Please provide a valid path.\")\nelse:\n    raise Exception(\"You must provide a path to the zip file.\")\n\n# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã —Å –ø–æ–º–æ—â—å—é resemble-enhance\n#!resemble-enhance --parallel_mode /content/drive/MyDrive/TTS_UZB/prepwavs/prepwavs /content/drive/MyDrive/TTS_UZB/prepwavs/wavs","metadata":{"executionInfo":{"elapsed":580,"status":"ok","timestamp":1710412695923,"user":{"displayName":"Ab As","userId":"09234410323733171058"},"user_tz":-300},"id":"PKLVRi1oRvfc","outputId":"152a87cc-8610-4b66-d9b8-c4f0bbc5a2ca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# –ü–µ—Ä–µ–º–µ—â–∞–µ–º—Å—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏\n%cd /content/drive/MyDrive/TTS_UZB/prepwavs/wavs\n\n# –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º –ø–∞–ø–∫—É —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏\nnew_zip_path = \"/content/drive/MyDrive/TTS_UZB/enhanced_wavs.zip\"\nwith zipfile.ZipFile(new_zip_path, 'w') as zip_f:\n    for root, dirs, files in os.walk(\".\"):\n        for file in files:\n            zip_f.write(os.path.join(root, file))\n\n# –ö–æ–ø–∏—Ä—É–µ–º –∞—Ä—Ö–∏–≤ –≤ Google Drive\ndestination_path = \"/content/drive/MyDrive/enhanced_wavs.zip\"\nshutil.copy(new_zip_path, destination_path)\n\nprint(f\"Enhanced dataset has been archived and copied to {destination_path}.\")","metadata":{"id":"b5iq-13XLglM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nfrom math import ceil\n\n# –ò—Å—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –æ—Ç–∫—É–¥–∞ –±—É–¥—É—Ç –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ñ–∞–π–ª—ã\nsource_dir = \"/kaggle/input/uzaudio/prepwavs\"\n# –ë–∞–∑–æ–≤–∞—è —Ü–µ–ª–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –∫—É–¥–∞ –±—É–¥—É—Ç –ø–æ–º–µ—â–∞—Ç—å—Å—è –Ω–æ–≤—ã–µ –ø–æ–¥–ø–∞–ø–∫–∏\nbase_target_dir = \"/kaggle/working/\"\n\n# –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–∞–ø–æ–∫ prepwavs1 –¥–æ prepwavs5\nnum_folders = 4\nfor i in range(1, num_folders + 1):\n    target_dir = os.path.join(base_target_dir, f\"prepwavs{i}\")\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–∞–ø–∫–µ\nfiles = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n\n# –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –∫–∞–∂–¥–æ–π –ø–∞–ø–∫–µ\nfiles_per_folder = ceil(len(files) / num_folders)\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º\nfor index, file_name in enumerate(files):\n    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–∞–ø–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞\n    target_folder_index = index // files_per_folder + 1  # +1 —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Å prepwavs1, –∞ –Ω–µ —Å prepwavs0\n    target_dir = os.path.join(base_target_dir, f\"prepwavs{target_folder_index}\")\n\n    # –ü—É—Ç—å –∫ —Ç–µ–∫—É—â–µ–º—É —Ñ–∞–π–ª—É –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–∞–ø–∫–µ\n    file_path = os.path.join(source_dir, file_name)\n    # –ü—É—Ç—å –∫ —Ü–µ–ª–µ–≤–æ–º—É —Ñ–∞–π–ª—É –≤ —Ü–µ–ª–µ–≤–æ–π –ø–∞–ø–∫–µ\n    target_path = os.path.join(target_dir, file_name)\n\n    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞\n    shutil.copy(file_path, target_path)\n\nprint(\"Files have been successfully distributed among the folders.\")\n","metadata":{"executionInfo":{"elapsed":127386,"status":"ok","timestamp":1710412972742,"user":{"displayName":"Ab As","userId":"09234410323733171058"},"user_tz":-300},"id":"fJnB34WgOGlg","outputId":"1c1b9f86-0d34-4a2c-a435-9293ae546bd6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/wavs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!resemble-enhance --device cpu --nfe 128 --lambd 0.5  --solver rk4 /content/sample_data/aud /content/sample_data/aud/out","metadata":{"id":"BKZyD8rxAifD","outputId":"34287df8-66c4-43c0-dee6-e5e095e2784a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get update && sudo apt-get install git-lfs\n!git lfs install\n\n","metadata":{"id":"GJlG3GvjByz4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import subprocess\nimport threading\nimport os\nimport shutil\nfrom math import ceil\ndef run_command(command):\n    subprocess.run(command, shell=True)\n\n# –ö–æ–º–∞–Ω–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\ncommands = [\n    \"resemble-enhance --parallel_mode /kaggle/working/prepwavs1 /kaggle/working/wavs\",\n    \"resemble-enhance --parallel_mode /kaggle/working/prepwavs2 /kaggle/working/wavs\",\n    \"resemble-enhance --parallel_mode /kaggle/working/prepwavs3 /kaggle/working/wavs\",\n    \"resemble-enhance --parallel_mode /kaggle/working/prepwavs4 /kaggle/working/wavs\"\n]\n\n# –ó–∞–ø—É—Å–∫–∞–µ–º –∫–∞–∂–¥—É—é –∫–æ–º–∞–Ω–¥—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ\nthreads = []\nfor cmd in commands:\n    thread = threading.Thread(target=run_command, args=(cmd,))\n    threads.append(thread)\n    thread.start()\n\n# –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤\nfor thread in threads:\n    thread.join()\n\nprint(\"All commands have been executed.\")\n","metadata":{"id":"JsGKTNPAOeog","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/wavs/wavs_archive.zip /kaggle/working/wavs/*\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls\n!cp /kaggle/working/wavs/wavs_archive.zip /kaggle/working/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/temp/\n%cd /kaggle/temp/\n!mkdir content\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:43:00.876050Z","iopub.execute_input":"2024-03-23T12:43:00.876979Z","iopub.status.idle":"2024-03-23T12:43:03.721394Z","shell.execute_reply.started":"2024-03-23T12:43:00.876947Z","shell.execute_reply":"2024-03-23T12:43:03.720273Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/temp\ncontent\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/temp/content\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:43:06.540602Z","iopub.execute_input":"2024-03-23T12:43:06.541576Z","iopub.status.idle":"2024-03-23T12:43:07.503878Z","shell.execute_reply.started":"2024-03-23T12:43:06.541538Z","shell.execute_reply":"2024-03-23T12:43:07.502837Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/temp/content\n","output_type":"stream"}]},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **Install software.** üì¶\n#@markdown ---\n#@markdown ####In this cell the synthesizer and its necessary dependencies to execute the training will be installed. (this may take a while)\n# –°–æ–∑–¥–∞–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ\n!python -m venv synthesizer_env\n\n# –ê–∫—Ç–∏–≤–∞—Ü–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –û–°\n# –î–ª—è Unix –∏–ª–∏ MacOS:\n!source synthesizer_env/bin/activate\n# clone:\n\n!git clone -q https://github.com/rmcpantoja/piper\n%cd /kaggle/temp/content/piper/src/python\n!wget -q \"https://raw.githubusercontent.com/coqui-ai/TTS/dev/TTS/bin/resample.py\"\n#!pip install -q -r requirements.txt\n!pip install -q cython>=0.29.0 piper-phonemize==1.1.0 librosa>=0.9.2 numpy==1.24 onnxruntime>=1.11.0 pytorch-lightning==1.7.7 torch==1.13.0+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n!pip install -q torchtext==0.14.0 torchvision==0.14.0\n# fixing recent compativility isswes:\n!pip install -q torchaudio==0.13.0 torchmetrics==0.11.4 faster_whisper\n!bash build_monotonic_align.sh\n# Useful vars:\nuse_whisper = False\nprint(\"Done!\")","metadata":{"cellView":"form","id":"_XwmTVlcUgCh","execution":{"iopub.status.busy":"2024-03-23T12:43:10.970344Z","iopub.execute_input":"2024-03-23T12:43:10.971156Z","iopub.status.idle":"2024-03-23T12:46:15.613226Z","shell.execute_reply.started":"2024-03-23T12:43:10.971118Z","shell.execute_reply":"2024-03-23T12:46:15.612128Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/temp/content/piper/src/python\n\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nalbumentations 1.4.0 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.24.0 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nseaborn 0.12.2 requires numpy!=1.24.0,>=1.17, but you have numpy 1.24.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntorchdata 0.7.1 requires torch>=2, but you have torch 1.13.0+cu117 which is incompatible.\nwoodwork 0.28.0 requires numpy<2.0.0,>=1.25.0, but you have numpy 1.24.0 which is incompatible.\nxarray 2024.2.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mCompiling /kaggle/temp/content/piper/src/python/piper_train/vits/monotonic_align/core.pyx because it changed.\n[1/1] Cythonizing /kaggle/temp/content/piper/src/python/piper_train/vits/monotonic_align/core.pyx\n/opt/conda/lib/python3.10/site-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /kaggle/temp/content/piper/src/python/piper_train/vits/monotonic_align/core.pyx\n  tree = Parsing.p_module(s, pxd, full_module_name)\nperformance hint: core.pyx:7:5: Exception check on 'maximum_path_each' will always require the GIL to be acquired.\nPossible solutions:\n\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\nperformance hint: core.pyx:38:6: Exception check on 'maximum_path_c' will always require the GIL to be acquired.\nPossible solutions:\n\t1. Declare 'maximum_path_c' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n\t2. Use an 'int' return type on 'maximum_path_c' to allow an error code to be returned.\nperformance hint: core.pyx:42:21: Exception check after calling 'maximum_path_each' will always require the GIL to be acquired.\nPossible solutions:\n\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\nDone!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color=\"ffc800\"> ü§ñ ***Training.*** ü§ñ","metadata":{"id":"A3bMzEE0V5Ma"}},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **1. Extract dataset.** üì•\n#@markdown ---\n#@markdown ####Important: the audios must be in <font color=\"orange\">**wav format, (16000 or 22050hz, 16-bits, mono), and, for convenience, numbered. Example:**\n\n#@markdown * <font color=\"orange\">**1.wav**</font>\n#@markdown * <font color=\"orange\">**2.wav**</font>\n#@markdown * <font color=\"orange\">**3.wav**</font>\n#@markdown * <font color=\"orange\">**.....**</font>\n\n#@markdown ---\nimport os\nimport wave\nimport zipfile\nimport datetime\n\ndef get_dataset_duration(wav_path):\n    totalduration = 0\n    for file_name in [x for x in os.listdir(wav_path) if os.path.isfile(x)]:\n        with wave.open(file_name, \"rb\") as wave_file:\n            frames = wave_file.getnframes()\n            rate = wave_file.getframerate()\n            duration = frames / float(rate)\n            totalduration += duration\n    wav_count = len(os.listdir(wav_path))\n    duration_str = str(datetime.timedelta(seconds=round(totalduration, 0)))\n    return wav_count, duration_str\n\nif not os.path.exists(\"/kaggle/temp/content/dataset\"):\n    os.makedirs(\"/kaggle/temp/content/dataset\")\n    os.makedirs(\"/kaggle/temp/content/dataset/wavs\")\n%cd /kaggle/temp/content\n#@markdown ### Audio dataset path to unzip:\n\n\n!find /kaggle/input/uzvoice3/wavs/home/alx/–ó–∞–≥—Ä—É–∑–∫–∏/wavs3/wavs -type f -name '*.wav' -print0 | xargs -0 cp -t /kaggle/temp/content/dataset/wavs/\n\n!cp /kaggle/input/uzvoice3/metadata.txt /kaggle/temp/content/dataset\n!mv /kaggle/temp/content/dataset/metadata.txt /kaggle/temp/content/dataset/metadata.csv\n%cd /kaggle/temp/content/dataset/wavs\naudio_count, dataset_dur = get_dataset_duration(\"/kaggle/temp/content/dataset/wavs\")\nprint(f\"Opened dataset with {audio_count} wavs with duration {dataset_dur}.\")\n%cd ..\n#@markdown ---","metadata":{"id":"SvEGjf0aV8eg","execution":{"iopub.status.busy":"2024-03-23T13:07:08.711907Z","iopub.execute_input":"2024-03-23T13:07:08.712709Z","iopub.status.idle":"2024-03-23T13:08:34.708851Z","shell.execute_reply.started":"2024-03-23T13:07:08.712674Z","shell.execute_reply":"2024-03-23T13:08:34.707680Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/temp/content\n/kaggle/temp/content/dataset/wavs\nOpened dataset with 10000 wavs with duration 10:16:53.\n/kaggle/temp/content/dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/temp/content/dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/temp/content/dataset/metadata.csv\n!cp /kaggle/input/metada/metadata.txt /kaggle/temp/content/dataset\n!mv /kaggle/temp/content/dataset/metadata.txt /kaggle/temp/content/dataset/metadata.csv\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **3. Preprocess dataset.** üîÑ\n#@markdown ---\nimport os\nuse_whisper = False\n#@markdown ### First of all, select the language of your dataset.\nlanguage = \"English (U.S.)\" #@param [\"ÿ£ŸÑÿπŸéÿ±Ÿéÿ®ŸêŸä\", \"Catal√†\", \"ƒçe≈°tina\", \"Dansk\", \"Deutsch\", \"ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨\", \"English (British)\", \"English (U.S.)\", \"Espa√±ol (Castellano)\", \"Espa√±ol (Latinoamericano)\", \"Suomi\", \"Fran√ßais\", \"Magyar\", \"Icelandic\", \"Italiano\", \"·É•·Éê·É†·Éó·É£·Éö·Éò\", \"“õ–∞–∑–∞“õ—à–∞\", \"L√´tzebuergesch\", \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä\", \"Nederlands\", \"Norsk\", \"Polski\", \"Portugu√™s (Brasil)\", \"Portugu√™s (Portugal)\", \"Rom√¢nƒÉ\", \"–†—É—Å—Å–∫–∏–π\", \"–°—Ä–ø—Å–∫–∏\", \"Svenska\", \"Kiswahili\", \"T√ºrk√ße\", \"—É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞\", \"Ti·∫øng Vi·ªát\", \"ÁÆÄ‰Ωì‰∏≠Êñá\"]\n#@markdown ---\n# language definition:\nlanguages = {\n    \"ÿ£ŸÑÿπŸéÿ±Ÿéÿ®ŸêŸä\": \"ar\",\n    \"Catal√†\": \"ca\",\n    \"ƒçe≈°tina\": \"cs\",\n    \"Dansk\": \"da\",\n    \"Deutsch\": \"de\",\n    \"ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨\": \"el\",\n    \"English (British)\": \"en\",\n    \"English (U.S.)\": \"en-us\",\n    \"Espa√±ol (Castellano)\": \"es\",\n    \"Espa√±ol (Latinoamericano)\": \"es-419\",\n    \"Suomi.\": \"fi\",\n    \"Fran√ßais\": \"fr\",\n    \"Magyar\": \"hu\",\n    \"Icelandic\": \"is\",\n    \"Italiano\": \"it\",\n    \"·É•·Éê·É†·Éó·É£·Éö·Éò\": \"ka\",\n    \"“õ–∞–∑–∞“õ—à–∞\": \"kk\",\n    \"L√´tzebuergesch\": \"lb\",\n    \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä\": \"ne\",\n    \"Nederlands\": \"nl\",\n    \"Norsk\": \"nb\",\n    \"Polski\": \"pl\",\n    \"Portugu√™s (Brasil)\": \"pt-br\",\n    \"Portugu√™s (Portugal)\": \"pt-pt\",\n    \"Rom√¢nƒÉ\": \"ro\",\n    \"–†—É—Å—Å–∫–∏–π\": \"ru\",\n    \"–°—Ä–ø—Å–∫–∏\": \"sr\",\n    \"Svenska\": \"sv\",\n    \"Kiswahili\": \"sw\",\n    \"T√ºrk√ße\": \"tr\",\n    \"—É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞\": \"uk\",\n    \"Ti·∫øng Vi·ªát\": \"vi\",\n    \"ÁÆÄ‰Ωì‰∏≠Êñá\": \"zh\"\n}\n\ndef _get_language(code):\n    return languages[code]\n\nfinal_language = _get_language(language)\n#@markdown ### Choose a name for your model:\nmodel_name = \"uzbekfemalehq22k\" #@param {type:\"string\"}\n#@markdown ---\n# output:\n#@markdown ### Choose the working folder: (recommended to save to Drive)\n\n#@markdown The working folder will be used in preprocessing, but also in training the model.\n!mkdir /kaggle/working/content/\n!mkdir /kaggle/working/content/drive\n!mkdir /kaggle/working/content/drive/MyDrive\n!mkdir /kaggle/working/content/drive/MyDrive/colab\n!mkdir /kaggle/working/content/drive/MyDrive/colab/piper\n!cp -rv /kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k /kaggle/working/content/drive/MyDrive/colab/piper\n\noutput_path = \"/kaggle/working/content/drive/MyDrive/colab/piper\" #@param {type:\"string\"}\noutput_dir = output_path+\"/\"+model_name\nif not os.path.exists(output_dir):\n  os.makedirs(output_dir)\n#@markdown ---\n#@markdown ### Choose dataset format:\ndataset_format = \"ljspeech\" #@param [\"ljspeech\", \"mycroft\"]\n#@markdown ---\n#@markdown ### Is this a single speaker dataset? Otherwise, uncheck:\nsingle_speaker = True #@param {type:\"boolean\"}\nif single_speaker:\n  force_sp = \" --single-speaker\"\nelse:\n  force_sp = \"\"\n#@markdown ---\n#@markdown ### Select the sample rate of the dataset:\nsample_rate = \"22050\" #@param [\"16000\", \"22050\"]\n#@markdown ---\n# creating paths:\nif not os.path.exists(\"/kaggle/temp/content/audio_cache\"):\n    os.makedirs(\"/kaggle/temp/content/audio_cache\")\n%cd /kaggle/temp/content/piper/src/python\n#@markdown ### Do you want to train using this sample rate, but your audios don't have it?\n#@markdown The resampler helps you do it quickly!\nresample = False #@param {type:\"boolean\"}\nif resample:\n  !python resample.py --input_dir \"/kaggle/temp/content/dataset/wavs\" --output_dir \"/kaggle/temp/content/dataset/wavs_resampled\" --output_sr {sample_rate} --file_ext \"wav\"\n  !for file in /kaggle/temp/content/dataset/wavs_resampled/*; do mv \"$file\" /kaggle/temp/content/dataset/wavs/; done\n#@markdown ---\n# check transcription:\nif use_whisper:\n    print(\"Transcript file hasn't been uploaded. Transcribing these audios using Whisper...\")\n    make_dataset(\"/kaggle/temp/content/dataset/wavs\", final_language[:2])\n    print(\"Transcription done! Pre-processing...\")\n!python -m piper_train.preprocess \\\n  --language {final_language} \\\n  --input-dir \"/kaggle/temp/content/dataset\" \\\n  --cache-dir \"/kaggle/temp/content/audio_cache\" \\\n  --output-dir \"{output_dir}\" \\\n  --dataset-name \"{model_name}\" \\\n  --dataset-format {dataset_format} \\\n  --sample-rate {sample_rate} \\\n  {force_sp}","metadata":{"cellView":"form","id":"dOyx9Y6JYvRF","execution":{"iopub.status.busy":"2024-03-23T13:10:19.204421Z","iopub.execute_input":"2024-03-23T13:10:19.204982Z","iopub.status.idle":"2024-03-23T13:20:45.565386Z","shell.execute_reply.started":"2024-03-23T13:10:19.204938Z","shell.execute_reply":"2024-03-23T13:20:45.564260Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory '/kaggle/working/content/': File exists\nmkdir: cannot create directory '/kaggle/working/content/drive': File exists\nmkdir: cannot create directory '/kaggle/working/content/drive/MyDrive': File exists\nmkdir: cannot create directory '/kaggle/working/content/drive/MyDrive/colab': File exists\nmkdir: cannot create directory '/kaggle/working/content/drive/MyDrive/colab/piper': File exists\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/dataset.jsonl' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/dataset.jsonl'\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/config.json' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/config.json'\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/events.out.tfevents.1710974923.c23aedc90c45.457.0' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/events.out.tfevents.1710974923.c23aedc90c45.457.0'\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/checkpoints/last.ckpt' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/checkpoints/last.ckpt'\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/hparams.yaml' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/hparams.yaml'\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_0/hparams.yaml' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_0/hparams.yaml'\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_0/events.out.tfevents.1710973783.c23aedc90c45.275.0' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_0/events.out.tfevents.1710973783.c23aedc90c45.275.0'\n'/kaggle/input/uzvoicecktp/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_0/checkpoints/last.ckpt' -> '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_0/checkpoints/last.ckpt'\n/kaggle/temp/content/piper/src/python\n\u001b[0;93m2024-03-23 13:10:43.292701615 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '131'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292758351 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292777976 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '139'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292794795 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '140'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292810901 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '134'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292927437 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '628'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292945993 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '623'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292959806 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '629'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292973299 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '620'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.292993907 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '625'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.319759085 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '131'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326333425 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '131'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326374535 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326391135 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '139'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326405506 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '140'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326419971 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '134'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326457681 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326481160 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '139'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326495506 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '140'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326509496 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '134'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326557338 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '628'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326573047 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '623'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326598719 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '629'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326612337 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '620'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326625428 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '625'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326749814 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '628'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326773813 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '623'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326786507 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '629'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326798322 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '620'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.326811044 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '625'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.340942465 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '131'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351252416 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351320277 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '139'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351339407 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '140'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351355380 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '134'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351556873 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '628'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351579331 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '623'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351595238 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '629'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351608851 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '620'. It is not used by any node and should be removed from the model.\u001b[m\n\u001b[0;93m2024-03-23 13:10:43.351621074 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '625'. It is not used by any node and should be removed from the model.\u001b[m\n","output_type":"stream"}]},{"cell_type":"code","source":"!cat /kaggle/temp/content/piper/notebooks/pretrained_models.json\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ipywidgets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **4. Settings.** üß∞\n#@markdown ---\nimport json\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport os\n#@markdown ### <font color=\"orange\">**Select the action to train this dataset: (READ CAREFULLY)**\n\n#@markdown * The option to <font color=\"orange\">continue a training</font> is self-explanatory. If you've previously trained a model with free colab, your time is up and you're considering training it some more, this is ideal for you. You just have to set the same settings that you set when you first trained this model.\n#@markdown * The option to <font color=\"orange\">convert a single-speaker model to a multi-speaker model</font> is self-explanatory, and for this it is important that you have processed a dataset that contains text and audio from all possible speakers that you want to train in your model.\n#@markdown * The <font color=\"orange\">finetune</font> option is used to train a dataset using a pretrained model, that is, train on that data. This option is ideal if you want to train a very small dataset (more than five minutes recommended).\n#@markdown * The <font color=\"orange\">train from scratch</font> option builds features such as dictionary and speech form from scratch, and this may take longer to converge. For this, hours of audio (8 at least) are recommended, which have a large collection of phonemes.\n\naction = \"Continue training\" #@param [\"Continue training\", \"convert single-speaker to multi-speaker model\", \"finetune\", \"train from scratch\"]\n#@markdown ---\nif action == \"Continue training\":\n    if os.path.exists(f\"{output_dir}/lightning_logs/version_1/checkpoints/last.ckpt\"):\n        ft_command = f'--resume_from_checkpoint \"{output_dir}/lightning_logs/version_1/checkpoints/last.ckpt\" '\n        print(f\"\\033[93mContinuing {model_name}'s training at: {output_dir}/lightning_logs/version_1/checkpoints/last.ckpt\")\n    else:\n        raise Exception(\"Training cannot be continued as there is no checkpoint to continue at.\")\nelif action == \"finetune\":\n    if os.path.exists(f\"{output_dir}/lightning_logs/version_1/checkpoints/last.ckpt\"):\n        raise Exception(\"Oh no! You have already trained this model before, you cannot choose this option since your progress will be lost, and then your previous time will not count. Please select the option to continue a training.\")\n    else:\n        ft_command = '--resume_from_checkpoint \"/kaggle/temp/content/pretrained.ckpt\" '\nelif action == \"convert single-speaker to multi-speaker model\":\n    if not single_speaker:\n        ft_command = '--resume_from_single_speaker_checkpoint \"/kaggle/temp/content/pretrained.ckpt\" '\n    else:\n        raise Exception(\"This dataset is not a multi-speaker dataset!\")\nelse:\n    ft_command = \"\"\nif action == \"convert single-speaker to multi-speaker model\" or action == \"finetune\":\n    def download_model(btn):\n        model_url = \"https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt\"  # –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç–æ –≤–∞—à–µ–π —Å—Å—ã–ª–∫–æ–π\n        print(\"\\033[93mDownloading pretrained model...\")\n        !wget -q \"{model_url}\" -O \"/kaggle/temp/content/pretrained.ckpt\"\n\n        if os.path.exists(\"/kaggle/temp/content/pretrained.ckpt\"):\n            print(\"\\033[93mModel downloaded!\")\n        else:\n            raise Exception(\"Couldn't download the pretrained model!\")\n    download_model(None)  # –í—ã–∑–æ–≤–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é –±–µ–∑ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏\n\nelse:\n    print(\"\\033[93mWarning: this model will be trained from scratch. You need at least 8 hours of data for everything to work decent. Good luck!\")\n#@markdown ### Choose batch size based on this dataset:\nbatch_size = 12 #@param {type:\"integer\"}\n#@markdown ---\n\n#@markdown ### Choose the quality for this model:\n\n#@markdown * x-low - 16Khz audio, 5-7M params\n#@markdown * medium - 22.05Khz audio, 15-20 params\n#@markdown * high - 22.05Khz audio, 28-32M params\nquality = \"medium\" #@param [\"high\", \"x-low\", \"medium\"]\n#@markdown ---\n#@markdown ### For how many epochs to save training checkpoints?\n#@markdown The larger your dataset, you should set this saving interval to a smaller value, as epochs can progress longer time.\ncheckpoint_epochs = 5 #@param {type:\"integer\"}\n#@markdown ---\n#@markdown ### Interval to save best k models:\n#@markdown Set to 0 if you want to disable saving multiple models. If this is the case, check the checkbox below. If set to 1, models will be saved with the file name epoch=xx-step=xx.ckpt, so you will need to empty Drive's trash every so often.\nnum_ckpt = 0 #@param {type:\"integer\"}\n#@markdown ---\n#@markdown ### Save latest model:\n#@markdown This checkbox must be checked if you want to save a single model (last.ckpt). Saving a single model is applied only if num_ckpt is equal to 0. If so, the interval parameter of epochs to save is ignored, since the last model per epoch is saved; also, you won't have to worry about storage. Being equal to 1, last.ckpt will be saved, but another model (model_vVersion.ckpt, the latter takes into account the epoch range you set), so you would have to empty the trash often.\n\n#@markdown **It's not recommended to use this option in extremely small datasets, since by saving the last model each epoch, this process will be very fast and the trainer will not be able to save the complete model, which would result in a corrupt last.ckpt.**\nsave_last = True # @param {type:\"boolean\"}\n#@markdown ---\n#@markdown ### Step interval to generate model samples:\nlog_every_n_steps = 1000 #@param {type:\"integer\"}\n#@markdown ---\n#@markdown ### Training epochs:\nmax_epochs = 10000 #@param {type:\"integer\"}\n#@markdown ---","metadata":{"cellView":"form","id":"ickQlOCRjkBL","execution":{"iopub.status.busy":"2024-03-23T13:33:35.739443Z","iopub.execute_input":"2024-03-23T13:33:35.739908Z","iopub.status.idle":"2024-03-23T13:33:35.845807Z","shell.execute_reply.started":"2024-03-23T13:33:35.739872Z","shell.execute_reply":"2024-03-23T13:33:35.844778Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\u001b[93mContinuing uzbekfemalehq22k's training at: /kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/checkpoints/last.ckpt\n\u001b[93mWarning: this model will be trained from scratch. You need at least 8 hours of data for everything to work decent. Good luck!\n","output_type":"stream"}]},{"cell_type":"code","source":"#@markdown # <font color=\"orange\"> **5. Run the TensorBoard extension.** üìà\n#@markdown ---\n#@markdown The TensorBoard is used to visualize the results of the model while it's being trained such as audio and losses.\n\n%load_ext tensorboard\n%tensorboard --logdir {output_dir}","metadata":{"cellView":"form","id":"MpKDfhAHjHJ3","execution":{"iopub.status.busy":"2024-03-23T13:33:42.773416Z","iopub.execute_input":"2024-03-23T13:33:42.774023Z","iopub.status.idle":"2024-03-23T13:33:58.327196Z","shell.execute_reply.started":"2024-03-23T13:33:42.773982Z","shell.execute_reply":"2024-03-23T13:33:58.326159Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-54c1b240bc7242d2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-54c1b240bc7242d2\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"!pip install numpy --upgrade\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:34:01.666051Z","iopub.execute_input":"2024-03-23T13:34:01.666901Z","iopub.status.idle":"2024-03-23T13:34:17.456048Z","shell.execute_reply.started":"2024-03-23T13:34:01.666865Z","shell.execute_reply":"2024-03-23T13:34:17.454813Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.24.0)\nCollecting numpy\n  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.0\n    Uninstalling numpy-1.24.0:\n      Successfully uninstalled numpy-1.24.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nxarray 2024.2.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n","output_type":"stream"}]},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **6. Train.** üèãÔ∏è‚Äç‚ôÇÔ∏è\n#@markdown ---\n#@markdown ### Run this cell to train your final model!\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"caching_allocator\"\n\n#@markdown ---\n#@markdown ### <font color=\"orange\">**Disable validation?**\n\n#@markdown By uncheck this checkbox, this will allow to train the full dataset, without using any audio files or examples as a validation set. So, it will not be able to generate audios on the tensorboard while it's training. It is recommended to disable validation on extremely small datasets.\nvalidation = True #@param {type:\"boolean\"}\nif validation:\n    validation_split = 0.01\n    num_test_examples = 1\nelse:\n    validation_split = 0\n    num_test_examples = 0\nif not save_last:\n    save_last_command = \"\"\nelse:\n    save_last_command = \"--save_last True \"\nget_ipython().system(f'''\npython -m piper_train \\\n--dataset-dir \"{output_dir}\" \\\n--accelerator 'gpu' \\\n--devices 2 \\\n--batch-size 12 \\\n--validation-split {validation_split} \\\n--num-test-examples {num_test_examples} \\\n--quality {quality} \\\n--checkpoint-epochs {checkpoint_epochs} \\\n--num_ckpt {num_ckpt} \\\n{save_last_command}\\\n--log_every_n_steps {log_every_n_steps} \\\n--max_epochs {max_epochs} \\\n{ft_command}\\\n--precision 32 \\\n--strategy ddp\n''')","metadata":{"cellView":"form","id":"X4zbSjXg2J3N","execution":{"iopub.status.busy":"2024-03-23T13:34:30.921873Z","iopub.execute_input":"2024-03-23T13:34:30.922837Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:52: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n  rank_zero_deprecation(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:731: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v2.0. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.\n  ckpt_path = ckpt_path or self.resume_from_checkpoint\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:345: UserWarning: The dirpath has changed from '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/checkpoints' to '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:345: UserWarning: The dirpath has changed from '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_1/checkpoints' to '/kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n  warnings.warn(\n2024-03-23 13:34:54.778404: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-23 13:34:54.778462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-23 13:34:54.780035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:98: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n  warning_cache.warn(\nEpoch: 2247. Steps: 1424098\nEpoch: 2247. Steps: 1424098\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2247. Steps: 1424098\nEpoch: 2247. Steps: 1424098\nwarning: audio amplitude out of range, auto clipped.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('loss_gen_all', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  warning_cache.warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('loss_disc_all', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  warning_cache.warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  warning_cache.warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (413) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [1, 9, 96], strides() = [48864, 96, 1]\nbucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [1, 9, 96], strides() = [47712, 96, 1]\nbucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nEpoch: 2248. Steps: 1424924\nEpoch: 2248. Steps: 1424924\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2248. Steps: 1424924\nEpoch: 2248. Steps: 1424924\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2248. Steps: 1424924\nEpoch: 2248. Steps: 1424924\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2248. Steps: 1424924\nEpoch: 2248. Steps: 1424924\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2248. Steps: 1424924\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:98: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n  warning_cache.warn(\nEpoch: 2248. Steps: 1424924\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2249. Steps: 1425750\nEpoch: 2249. Steps: 1425750\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2249. Steps: 1425750\nEpoch: 2249. Steps: 1425750\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2249. Steps: 1425750\nEpoch: 2249. Steps: 1425750\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2249. Steps: 1425750\nEpoch: 2249. Steps: 1425750\nEpoch: 2249. Steps: 1425750\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2249. Steps: 1425750\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2250. Steps: 1426576\nEpoch: 2250. Steps: 1426576\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2250. Steps: 1426576\nEpoch: 2250. Steps: 1426576\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2250. Steps: 1426576\nEpoch: 2250. Steps: 1426576\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2250. Steps: 1426576\nEpoch: 2250. Steps: 1426576\nEpoch: 2250. Steps: 1426576\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2250. Steps: 1426576\nEpoch: 2251. Steps: 1427402\nEpoch: 2251. Steps: 1427402\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2251. Steps: 1427402\nEpoch: 2251. Steps: 1427402\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2251. Steps: 1427402\nEpoch: 2251. Steps: 1427402\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2251. Steps: 1427402\nEpoch: 2251. Steps: 1427402\nEpoch: 2251. Steps: 1427402\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2251. Steps: 1427402\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2252. Steps: 1428228\nEpoch: 2252. Steps: 1428228\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2252. Steps: 1428228\nEpoch: 2252. Steps: 1428228\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2252. Steps: 1428228\nEpoch: 2252. Steps: 1428228\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2252. Steps: 1428228\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2252. Steps: 1428228\nEpoch: 2252. Steps: 1428228\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2252. Steps: 1428228\nEpoch: 2253. Steps: 1429054\nEpoch: 2253. Steps: 1429054\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2253. Steps: 1429054\nEpoch: 2253. Steps: 1429054\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2253. Steps: 1429054\nEpoch: 2253. Steps: 1429054\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2253. Steps: 1429054\nEpoch: 2253. Steps: 1429054\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2253. Steps: 1429054\nEpoch: 2253. Steps: 1429054\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2254. Steps: 1429880\nEpoch: 2254. Steps: 1429880\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2254. Steps: 1429880\nEpoch: 2254. Steps: 1429880\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2254. Steps: 1429880\nEpoch: 2254. Steps: 1429880\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2254. Steps: 1429880\nEpoch: 2254. Steps: 1429880\nEpoch: 2254. Steps: 1429880\nEpoch: 2254. Steps: 1429880\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2255. Steps: 1430706\nEpoch: 2255. Steps: 1430706\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2255. Steps: 1430706\nEpoch: 2255. Steps: 1430706\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2255. Steps: 1430706\nEpoch: 2255. Steps: 1430706\nEpoch: 2255. Steps: 1430706\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2255. Steps: 1430706\nEpoch: 2255. Steps: 1430706\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2255. Steps: 1430706\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2256. Steps: 1431532\nEpoch: 2256. Steps: 1431532\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2256. Steps: 1431532\nEpoch: 2256. Steps: 1431532\nEpoch: 2256. Steps: 1431532\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2256. Steps: 1431532\nEpoch: 2256. Steps: 1431532\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2256. Steps: 1431532\nEpoch: 2256. Steps: 1431532\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2256. Steps: 1431532\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2257. Steps: 1432358\nEpoch: 2257. Steps: 1432358\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2257. Steps: 1432358\nEpoch: 2257. Steps: 1432358\nEpoch: 2257. Steps: 1432358\nEpoch: 2257. Steps: 1432358\nEpoch: 2257. Steps: 1432358\nEpoch: 2257. Steps: 1432358\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2257. Steps: 1432358\nEpoch: 2257. Steps: 1432358\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2258. Steps: 1433184\nEpoch: 2258. Steps: 1433184\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2258. Steps: 1433184\nEpoch: 2258. Steps: 1433184\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2258. Steps: 1433184\nEpoch: 2258. Steps: 1433184\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2258. Steps: 1433184\nEpoch: 2258. Steps: 1433184\nEpoch: 2258. Steps: 1433184\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2258. Steps: 1433184\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2259. Steps: 1434010\nEpoch: 2259. Steps: 1434010\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2259. Steps: 1434010\nEpoch: 2259. Steps: 1434010\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2259. Steps: 1434010\nEpoch: 2259. Steps: 1434010\nEpoch: 2259. Steps: 1434010\nEpoch: 2259. Steps: 1434010\nEpoch: 2259. Steps: 1434010\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2259. Steps: 1434010\nEpoch: 2260. Steps: 1434836\nEpoch: 2260. Steps: 1434836\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2260. Steps: 1434836\nEpoch: 2260. Steps: 1434836\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2260. Steps: 1434836\nEpoch: 2260. Steps: 1434836\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2260. Steps: 1434836\nEpoch: 2260. Steps: 1434836\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2260. Steps: 1434836\nEpoch: 2260. Steps: 1434836\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nEpoch: 2261. Steps: 1435662\nEpoch: 2262. Steps: 1436488\nEpoch: 2262. Steps: 1436488\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2262. Steps: 1436488\nEpoch: 2262. Steps: 1436488\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2262. Steps: 1436488\nEpoch: 2262. Steps: 1436488\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2262. Steps: 1436488\nEpoch: 2262. Steps: 1436488\nEpoch: 2262. Steps: 1436488\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2262. Steps: 1436488\nEpoch: 2263. Steps: 1437314\nEpoch: 2263. Steps: 1437314\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2263. Steps: 1437314\nEpoch: 2263. Steps: 1437314\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2263. Steps: 1437314\nEpoch: 2263. Steps: 1437314\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2263. Steps: 1437314\nEpoch: 2263. Steps: 1437314\nEpoch: 2263. Steps: 1437314\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2263. Steps: 1437314\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2264. Steps: 1438140\nEpoch: 2264. Steps: 1438140\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2264. Steps: 1438140\nEpoch: 2264. Steps: 1438140\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2264. Steps: 1438140\nEpoch: 2264. Steps: 1438140\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2264. Steps: 1438140\nEpoch: 2264. Steps: 1438140\nEpoch: 2264. Steps: 1438140\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2264. Steps: 1438140\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2265. Steps: 1438966\nEpoch: 2265. Steps: 1438966\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2265. Steps: 1438966\nEpoch: 2265. Steps: 1438966\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2265. Steps: 1438966\nEpoch: 2265. Steps: 1438966\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2265. Steps: 1438966\nEpoch: 2265. Steps: 1438966\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2265. Steps: 1438966\nEpoch: 2265. Steps: 1438966\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2266. Steps: 1439792\nEpoch: 2266. Steps: 1439792\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2266. Steps: 1439792\nEpoch: 2266. Steps: 1439792\nEpoch: 2266. Steps: 1439792\nEpoch: 2266. Steps: 1439792\nEpoch: 2266. Steps: 1439792\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2266. Steps: 1439792\nEpoch: 2266. Steps: 1439792\nEpoch: 2266. Steps: 1439792\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2267. Steps: 1440618\nEpoch: 2267. Steps: 1440618\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2267. Steps: 1440618\nEpoch: 2267. Steps: 1440618\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2267. Steps: 1440618\nEpoch: 2267. Steps: 1440618\nEpoch: 2267. Steps: 1440618\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2267. Steps: 1440618\nEpoch: 2267. Steps: 1440618\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2267. Steps: 1440618\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2268. Steps: 1441444\nEpoch: 2268. Steps: 1441444\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2268. Steps: 1441444\nEpoch: 2268. Steps: 1441444\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2268. Steps: 1441444\nEpoch: 2268. Steps: 1441444\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2268. Steps: 1441444\nEpoch: 2268. Steps: 1441444\nEpoch: 2268. Steps: 1441444\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2268. Steps: 1441444\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2269. Steps: 1442270\nEpoch: 2269. Steps: 1442270\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2269. Steps: 1442270\nEpoch: 2269. Steps: 1442270\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2269. Steps: 1442270\nEpoch: 2269. Steps: 1442270\nEpoch: 2269. Steps: 1442270\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2269. Steps: 1442270\nEpoch: 2269. Steps: 1442270\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2269. Steps: 1442270\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2270. Steps: 1443096\nEpoch: 2270. Steps: 1443096\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2270. Steps: 1443096\nEpoch: 2270. Steps: 1443096\nEpoch: 2270. Steps: 1443096\nEpoch: 2270. Steps: 1443096\nEpoch: 2270. Steps: 1443096\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2270. Steps: 1443096\nEpoch: 2270. Steps: 1443096\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2270. Steps: 1443096\nEpoch: 2271. Steps: 1443922\nEpoch: 2271. Steps: 1443922\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2271. Steps: 1443922\nEpoch: 2271. Steps: 1443922\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2271. Steps: 1443922\nEpoch: 2271. Steps: 1443922\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2271. Steps: 1443922\nEpoch: 2271. Steps: 1443922\nEpoch: 2271. Steps: 1443922\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2271. Steps: 1443922\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2272. Steps: 1444748\nEpoch: 2272. Steps: 1444748\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2272. Steps: 1444748\nEpoch: 2272. Steps: 1444748\nEpoch: 2272. Steps: 1444748\nEpoch: 2272. Steps: 1444748\nEpoch: 2272. Steps: 1444748\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2272. Steps: 1444748\nEpoch: 2272. Steps: 1444748\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2272. Steps: 1444748\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2273. Steps: 1445574\nEpoch: 2273. Steps: 1445574\nEpoch: 2273. Steps: 1445574\nEpoch: 2273. Steps: 1445574\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2273. Steps: 1445574\nEpoch: 2273. Steps: 1445574\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2273. Steps: 1445574\nEpoch: 2273. Steps: 1445574\nEpoch: 2273. Steps: 1445574\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2273. Steps: 1445574\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2274. Steps: 1446400\nEpoch: 2274. Steps: 1446400\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2274. Steps: 1446400\nEpoch: 2274. Steps: 1446400\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2274. Steps: 1446400\nEpoch: 2274. Steps: 1446400\nEpoch: 2274. Steps: 1446400\nEpoch: 2274. Steps: 1446400\nEpoch: 2274. Steps: 1446400\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2274. Steps: 1446400\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2275. Steps: 1447226\nEpoch: 2275. Steps: 1447226\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2275. Steps: 1447226\nEpoch: 2275. Steps: 1447226\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2275. Steps: 1447226\nEpoch: 2275. Steps: 1447226\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2275. Steps: 1447226\nEpoch: 2275. Steps: 1447226\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2275. Steps: 1447226\nEpoch: 2275. Steps: 1447226\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2276. Steps: 1448052\nEpoch: 2276. Steps: 1448052\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2276. Steps: 1448052\nEpoch: 2276. Steps: 1448052\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2276. Steps: 1448052\nEpoch: 2276. Steps: 1448052\nEpoch: 2276. Steps: 1448052\nEpoch: 2276. Steps: 1448052\nEpoch: 2276. Steps: 1448052\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2276. Steps: 1448052\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2277. Steps: 1448878\nEpoch: 2277. Steps: 1448878\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2277. Steps: 1448878\nEpoch: 2277. Steps: 1448878\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2277. Steps: 1448878\nEpoch: 2277. Steps: 1448878\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2277. Steps: 1448878\nEpoch: 2277. Steps: 1448878\nEpoch: 2277. Steps: 1448878\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2277. Steps: 1448878\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2278. Steps: 1449704\nEpoch: 2278. Steps: 1449704\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2278. Steps: 1449704\nEpoch: 2278. Steps: 1449704\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2278. Steps: 1449704\nEpoch: 2278. Steps: 1449704\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2278. Steps: 1449704\nEpoch: 2278. Steps: 1449704\nEpoch: 2278. Steps: 1449704\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2278. Steps: 1449704\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2279. Steps: 1450530\nEpoch: 2279. Steps: 1450530\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2279. Steps: 1450530\nEpoch: 2279. Steps: 1450530\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2279. Steps: 1450530\nEpoch: 2279. Steps: 1450530\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2279. Steps: 1450530\nEpoch: 2279. Steps: 1450530\nEpoch: 2279. Steps: 1450530\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2279. Steps: 1450530\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2280. Steps: 1451356\nEpoch: 2280. Steps: 1451356\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2280. Steps: 1451356\nEpoch: 2280. Steps: 1451356\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2280. Steps: 1451356\nEpoch: 2280. Steps: 1451356\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2280. Steps: 1451356\nEpoch: 2280. Steps: 1451356\nEpoch: 2280. Steps: 1451356\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2280. Steps: 1451356\nEpoch: 2281. Steps: 1452182\nEpoch: 2281. Steps: 1452182\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2281. Steps: 1452182\nEpoch: 2281. Steps: 1452182\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2281. Steps: 1452182\nEpoch: 2281. Steps: 1452182\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2281. Steps: 1452182\nEpoch: 2281. Steps: 1452182\nEpoch: 2281. Steps: 1452182\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2281. Steps: 1452182\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2282. Steps: 1453008\nEpoch: 2283. Steps: 1453834\nEpoch: 2283. Steps: 1453834\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2283. Steps: 1453834\nEpoch: 2283. Steps: 1453834\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2283. Steps: 1453834\nEpoch: 2283. Steps: 1453834\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2283. Steps: 1453834\nEpoch: 2283. Steps: 1453834\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2283. Steps: 1453834\nEpoch: 2283. Steps: 1453834\nEpoch: 2284. Steps: 1454660\nEpoch: 2284. Steps: 1454660\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2284. Steps: 1454660\nEpoch: 2284. Steps: 1454660\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2284. Steps: 1454660\nEpoch: 2284. Steps: 1454660\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2284. Steps: 1454660\nEpoch: 2284. Steps: 1454660\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2284. Steps: 1454660\nEpoch: 2284. Steps: 1454660\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2285. Steps: 1455486\nEpoch: 2285. Steps: 1455486\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2285. Steps: 1455486\nEpoch: 2285. Steps: 1455486\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2285. Steps: 1455486\nEpoch: 2285. Steps: 1455486\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2285. Steps: 1455486\nEpoch: 2285. Steps: 1455486\nEpoch: 2285. Steps: 1455486\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2285. Steps: 1455486\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2286. Steps: 1456312\nEpoch: 2286. Steps: 1456312\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2286. Steps: 1456312\nEpoch: 2286. Steps: 1456312\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2286. Steps: 1456312\nEpoch: 2286. Steps: 1456312\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2286. Steps: 1456312\nEpoch: 2286. Steps: 1456312\nEpoch: 2286. Steps: 1456312\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2286. Steps: 1456312\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2287. Steps: 1457138\nEpoch: 2287. Steps: 1457138\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2287. Steps: 1457138\nEpoch: 2287. Steps: 1457138\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2287. Steps: 1457138\nEpoch: 2287. Steps: 1457138\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2287. Steps: 1457138\nEpoch: 2287. Steps: 1457138\nEpoch: 2287. Steps: 1457138\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2287. Steps: 1457138\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2288. Steps: 1457964\nEpoch: 2288. Steps: 1457964\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2288. Steps: 1457964\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2288. Steps: 1457964\nEpoch: 2288. Steps: 1457964\nEpoch: 2288. Steps: 1457964\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2288. Steps: 1457964\nEpoch: 2288. Steps: 1457964\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2288. Steps: 1457964\nEpoch: 2288. Steps: 1457964\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2289. Steps: 1458790\nEpoch: 2289. Steps: 1458790\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2289. Steps: 1458790\nEpoch: 2289. Steps: 1458790\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2289. Steps: 1458790\nEpoch: 2289. Steps: 1458790\nEpoch: 2289. Steps: 1458790\nEpoch: 2289. Steps: 1458790\nEpoch: 2289. Steps: 1458790\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2289. Steps: 1458790\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2290. Steps: 1459616\nEpoch: 2290. Steps: 1459616\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2290. Steps: 1459616\nEpoch: 2290. Steps: 1459616\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2290. Steps: 1459616\nEpoch: 2290. Steps: 1459616\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2290. Steps: 1459616\nEpoch: 2290. Steps: 1459616\nEpoch: 2290. Steps: 1459616\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2290. Steps: 1459616\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2291. Steps: 1460442\nEpoch: 2291. Steps: 1460442\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2291. Steps: 1460442\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2291. Steps: 1460442\nEpoch: 2291. Steps: 1460442\nEpoch: 2291. Steps: 1460442\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2291. Steps: 1460442\nEpoch: 2291. Steps: 1460442\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2291. Steps: 1460442\nEpoch: 2291. Steps: 1460442\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2292. Steps: 1461268\nEpoch: 2292. Steps: 1461268\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2292. Steps: 1461268\nEpoch: 2292. Steps: 1461268\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2292. Steps: 1461268\nEpoch: 2292. Steps: 1461268\nEpoch: 2292. Steps: 1461268\nEpoch: 2292. Steps: 1461268\nEpoch: 2292. Steps: 1461268\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2292. Steps: 1461268\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2293. Steps: 1462094\nEpoch: 2293. Steps: 1462094\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2293. Steps: 1462094\nEpoch: 2293. Steps: 1462094\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2293. Steps: 1462094\nEpoch: 2293. Steps: 1462094\nEpoch: 2293. Steps: 1462094\nEpoch: 2293. Steps: 1462094\nEpoch: 2293. Steps: 1462094\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2293. Steps: 1462094\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2294. Steps: 1462920\nEpoch: 2294. Steps: 1462920\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2294. Steps: 1462920\nEpoch: 2294. Steps: 1462920\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2294. Steps: 1462920\nEpoch: 2294. Steps: 1462920\nEpoch: 2294. Steps: 1462920\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2294. Steps: 1462920\nEpoch: 2294. Steps: 1462920\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2294. Steps: 1462920\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2295. Steps: 1463746\nEpoch: 2295. Steps: 1463746\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2295. Steps: 1463746\nEpoch: 2295. Steps: 1463746\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2295. Steps: 1463746\nEpoch: 2295. Steps: 1463746\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2295. Steps: 1463746\nEpoch: 2295. Steps: 1463746\nEpoch: 2295. Steps: 1463746\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2295. Steps: 1463746\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2296. Steps: 1464572\nEpoch: 2296. Steps: 1464572\nEpoch: 2296. Steps: 1464572\nEpoch: 2296. Steps: 1464572\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2296. Steps: 1464572\nEpoch: 2296. Steps: 1464572\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2296. Steps: 1464572\nEpoch: 2296. Steps: 1464572\nEpoch: 2296. Steps: 1464572\nEpoch: 2296. Steps: 1464572\nEpoch: 2297. Steps: 1465398\nEpoch: 2297. Steps: 1465398\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2297. Steps: 1465398\nEpoch: 2297. Steps: 1465398\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2297. Steps: 1465398\nEpoch: 2297. Steps: 1465398\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2297. Steps: 1465398\nEpoch: 2297. Steps: 1465398\nEpoch: 2297. Steps: 1465398\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2297. Steps: 1465398\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2298. Steps: 1466224\nEpoch: 2298. Steps: 1466224\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2298. Steps: 1466224\nEpoch: 2298. Steps: 1466224\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2298. Steps: 1466224\nEpoch: 2298. Steps: 1466224\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2298. Steps: 1466224\nEpoch: 2298. Steps: 1466224\nEpoch: 2298. Steps: 1466224\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2298. Steps: 1466224\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2299. Steps: 1467050\nEpoch: 2299. Steps: 1467050\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2299. Steps: 1467050\nEpoch: 2299. Steps: 1467050\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2299. Steps: 1467050\nEpoch: 2299. Steps: 1467050\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2299. Steps: 1467050\nEpoch: 2299. Steps: 1467050\nEpoch: 2299. Steps: 1467050\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2299. Steps: 1467050\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2300. Steps: 1467876\nEpoch: 2300. Steps: 1467876\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2300. Steps: 1467876\nEpoch: 2300. Steps: 1467876\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2300. Steps: 1467876\nEpoch: 2300. Steps: 1467876\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2300. Steps: 1467876\nEpoch: 2300. Steps: 1467876\nEpoch: 2300. Steps: 1467876\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2300. Steps: 1467876\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2301. Steps: 1468702\nEpoch: 2301. Steps: 1468702\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2301. Steps: 1468702\nEpoch: 2301. Steps: 1468702\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2301. Steps: 1468702\nEpoch: 2301. Steps: 1468702\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2301. Steps: 1468702\nEpoch: 2301. Steps: 1468702\nEpoch: 2301. Steps: 1468702\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2301. Steps: 1468702\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2302. Steps: 1469528\nEpoch: 2302. Steps: 1469528\nEpoch: 2302. Steps: 1469528\nEpoch: 2302. Steps: 1469528\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2302. Steps: 1469528\nEpoch: 2302. Steps: 1469528\nEpoch: 2302. Steps: 1469528\nEpoch: 2302. Steps: 1469528\nEpoch: 2302. Steps: 1469528\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2302. Steps: 1469528\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2303. Steps: 1470354\nEpoch: 2303. Steps: 1470354\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2303. Steps: 1470354\nEpoch: 2303. Steps: 1470354\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2303. Steps: 1470354\nEpoch: 2303. Steps: 1470354\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2303. Steps: 1470354\nEpoch: 2303. Steps: 1470354\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2303. Steps: 1470354\nEpoch: 2303. Steps: 1470354\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2304. Steps: 1471180\nEpoch: 2304. Steps: 1471180\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2304. Steps: 1471180\nEpoch: 2304. Steps: 1471180\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2304. Steps: 1471180\nEpoch: 2304. Steps: 1471180\nEpoch: 2304. Steps: 1471180\nEpoch: 2304. Steps: 1471180\nEpoch: 2304. Steps: 1471180\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2304. Steps: 1471180\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nEpoch: 2305. Steps: 1472006\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2306. Steps: 1472832\nEpoch: 2306. Steps: 1472832\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2306. Steps: 1472832\nEpoch: 2306. Steps: 1472832\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2306. Steps: 1472832\nEpoch: 2306. Steps: 1472832\nEpoch: 2306. Steps: 1472832\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2306. Steps: 1472832\nEpoch: 2306. Steps: 1472832\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2306. Steps: 1472832\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2307. Steps: 1473658\nEpoch: 2307. Steps: 1473658\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2307. Steps: 1473658\nEpoch: 2307. Steps: 1473658\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2307. Steps: 1473658\nEpoch: 2307. Steps: 1473658\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2307. Steps: 1473658\nEpoch: 2307. Steps: 1473658\nEpoch: 2307. Steps: 1473658\nEpoch: 2307. Steps: 1473658\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2308. Steps: 1474484\nEpoch: 2308. Steps: 1474484\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2308. Steps: 1474484\nEpoch: 2308. Steps: 1474484\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2308. Steps: 1474484\nEpoch: 2308. Steps: 1474484\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2308. Steps: 1474484\nEpoch: 2308. Steps: 1474484\nEpoch: 2308. Steps: 1474484\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2308. Steps: 1474484\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2309. Steps: 1475310\nEpoch: 2309. Steps: 1475310\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2309. Steps: 1475310\nEpoch: 2309. Steps: 1475310\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2309. Steps: 1475310\nEpoch: 2309. Steps: 1475310\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2309. Steps: 1475310\nEpoch: 2309. Steps: 1475310\nEpoch: 2309. Steps: 1475310\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2309. Steps: 1475310\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2310. Steps: 1476136\nEpoch: 2310. Steps: 1476136\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2310. Steps: 1476136\nEpoch: 2310. Steps: 1476136\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2310. Steps: 1476136\nEpoch: 2310. Steps: 1476136\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2310. Steps: 1476136\nEpoch: 2310. Steps: 1476136\nEpoch: 2310. Steps: 1476136\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2310. Steps: 1476136\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2311. Steps: 1476962\nEpoch: 2311. Steps: 1476962\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2311. Steps: 1476962\nEpoch: 2311. Steps: 1476962\nEpoch: 2311. Steps: 1476962\nEpoch: 2311. Steps: 1476962\nEpoch: 2311. Steps: 1476962\nEpoch: 2311. Steps: 1476962\nEpoch: 2311. Steps: 1476962\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2311. Steps: 1476962\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2312. Steps: 1477788\nEpoch: 2312. Steps: 1477788\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2312. Steps: 1477788\nEpoch: 2312. Steps: 1477788\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2312. Steps: 1477788\nEpoch: 2312. Steps: 1477788\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2312. Steps: 1477788\nEpoch: 2312. Steps: 1477788\nEpoch: 2312. Steps: 1477788\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2312. Steps: 1477788\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2313. Steps: 1478614\nEpoch: 2313. Steps: 1478614\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2313. Steps: 1478614\nEpoch: 2313. Steps: 1478614\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2313. Steps: 1478614\nEpoch: 2313. Steps: 1478614\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2313. Steps: 1478614\nEpoch: 2313. Steps: 1478614\nEpoch: 2313. Steps: 1478614\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2313. Steps: 1478614\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2314. Steps: 1479440\nEpoch: 2314. Steps: 1479440\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2314. Steps: 1479440\nEpoch: 2314. Steps: 1479440\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2314. Steps: 1479440\nEpoch: 2314. Steps: 1479440\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2314. Steps: 1479440\nEpoch: 2314. Steps: 1479440\nEpoch: 2314. Steps: 1479440\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2314. Steps: 1479440\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2315. Steps: 1480266\nEpoch: 2315. Steps: 1480266\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2315. Steps: 1480266\nEpoch: 2315. Steps: 1480266\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2315. Steps: 1480266\nEpoch: 2315. Steps: 1480266\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2315. Steps: 1480266\nEpoch: 2315. Steps: 1480266\nEpoch: 2315. Steps: 1480266\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2315. Steps: 1480266\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nEpoch: 2316. Steps: 1481092\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2316. Steps: 1481092\nEpoch: 2317. Steps: 1481918\nEpoch: 2317. Steps: 1481918\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2317. Steps: 1481918\nEpoch: 2317. Steps: 1481918\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2317. Steps: 1481918\nEpoch: 2317. Steps: 1481918\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2317. Steps: 1481918\nEpoch: 2317. Steps: 1481918\nEpoch: 2317. Steps: 1481918\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2317. Steps: 1481918\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2318. Steps: 1482744\nEpoch: 2318. Steps: 1482744\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2318. Steps: 1482744\nEpoch: 2318. Steps: 1482744\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2318. Steps: 1482744\nEpoch: 2318. Steps: 1482744\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2318. Steps: 1482744\nEpoch: 2318. Steps: 1482744\nEpoch: 2318. Steps: 1482744\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2318. Steps: 1482744\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2319. Steps: 1483570\nEpoch: 2319. Steps: 1483570\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2319. Steps: 1483570\nEpoch: 2319. Steps: 1483570\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2319. Steps: 1483570\nEpoch: 2319. Steps: 1483570\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2319. Steps: 1483570\nEpoch: 2319. Steps: 1483570\nEpoch: 2319. Steps: 1483570\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2319. Steps: 1483570\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2320. Steps: 1484396\nEpoch: 2320. Steps: 1484396\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2320. Steps: 1484396\nEpoch: 2320. Steps: 1484396\nEpoch: 2320. Steps: 1484396\nEpoch: 2320. Steps: 1484396\nEpoch: 2320. Steps: 1484396\nEpoch: 2320. Steps: 1484396\nEpoch: 2320. Steps: 1484396\nwarning: audio amplitude out of range, auto clipped.\nEpoch: 2320. Steps: 1484396\nwarning: audio amplitude out of range, auto clipped.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/working/content/drive/MyDrive/colab/piper/uzbekfemalehq22k/lightning_logs/version_4/checkpoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  <font color=\"orange\">**Have you finished training and want to test the model?**\n\n* If you want to run this model in any software that Piper integrates or the same Piper app, export your model using the [model exporter notebook](https://colab.research.google.com/github/rmcpantoja/piper/blob/master/notebooks/piper_model_exporter.ipynb)!\n* Wait! I want to test this right now before exporting it to the supported format for Piper. Test your generated last.ckpt with [this notebook](https://colab.research.google.com/github/rmcpantoja/piper/blob/master/notebooks/piper_inference_(ckpt).ipynb)!","metadata":{"id":"6ISG085SYn85"}}]}